{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robust Error Handling for CSV File Processing\n",
    "\n",
    "Reading and processing CSV (Comma-Separated Values) files in production environments presents numerous challenges that can compromise data integrity and system stability. CSV files, despite their simple format, often arrive from various sources with inconsistent formatting, encoding issues, and data quality problems. Common issues include malformed data, missing columns, incorrect delimiters, mixed data types, and encoding variations. These files may also be corrupted, truncated, or too large for available memory. Additionally, system-level issues such as insufficient permissions, network interruptions during file transfers, or concurrent access attempts can further complicate the reading process. Current CSV reading implementations often handle only basic error cases, leading to unexpected crashes, data corruption, or silent failures that are difficult to diagnose and debug."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "Design and implement a comprehensive error handling system for CSV file processing that:\n",
    "\n",
    "1. Identifies and appropriately responds to all potential failure points in the CSV reading pipeline\n",
    "2. Provides detailed, actionable error messages that facilitate quick problem resolution\n",
    "3. Implements robust logging mechanisms for error tracking and system monitoring\n",
    "4. Manages system resources effectively, particularly when dealing with large files\n",
    "5. Preserves data integrity through proper validation and sanitization\n",
    "6. Enables graceful degradation and recovery options where possible\n",
    "7. Maintains processing efficiency while incorporating these safety mechanisms\n",
    "\n",
    "The solution must handle both technical errors (file system issues, memory constraints) and data-related errors (format problems, validation failures) while remaining maintainable and adaptable to different business requirements. It should strike a balance between being thorough enough to catch all critical issues and efficient enough to not significantly impact performance during normal operation.\n",
    "\n",
    "### Success Criteria\n",
    "The implementation will be considered successful if it:\n",
    "\n",
    "* Prevents all unhandled exceptions from reaching the end user\n",
    "* Reduces system crashes due to CSV processing by 99%\n",
    "* Maintains processing speed within 10% of baseline performance\n",
    "* Provides error messages that lead to resolution within one debugging cycle\n",
    "* Achieves 100% error detection rate for defined error categories\n",
    "* Enables recovery from at least 80% of non-critical errors\n",
    "* Requires minimal configuration for common use cases while remaining flexible for specific requirements\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical Requirements: CSV Error Handling System\n",
    "\n",
    "## 1. File System Requirements\n",
    "\n",
    "### 1.1 File Access and Permissions\n",
    "- Must handle files up to 10GB in size\n",
    "- Support concurrent read access from multiple processes\n",
    "- Handle file system permissions (read/write/execute)\n",
    "- Support different file systems (NTFS, ext4, FAT32)\n",
    "- Handle network-mounted filesystems (NFS, SMB)\n",
    "- Implement file locking mechanisms for concurrent access\n",
    "- Support relative and absolute file paths\n",
    "- Handle symbolic links and shortcuts\n",
    "\n",
    "### 1.2 File Format Requirements\n",
    "- Support multiple CSV variants:\n",
    "  - Comma-separated (,)\n",
    "  - Tab-separated (\\t)\n",
    "  - Semicolon-separated (;)\n",
    "  - Custom delimiters\n",
    "- Handle line endings: \\n, \\r\\n, \\r\n",
    "- Support quoted fields with embedded delimiters\n",
    "- Handle BOM (Byte Order Mark) in UTF files\n",
    "- Support compressed files (.gz, .zip)\n",
    "- Handle missing or empty files gracefully\n",
    "\n",
    "### 1.3 Encoding Requirements\n",
    "- Primary support for UTF-8\n",
    "- Fallback support for:\n",
    "  - ASCII\n",
    "  - UTF-16 (both BE and LE)\n",
    "  - ISO-8859-1\n",
    "  - Windows-1252\n",
    "  - Custom encodings\n",
    "- Auto-detection of file encoding\n",
    "- Handling of mixed encodings within a file\n",
    "- Support for non-printable characters\n",
    "\n",
    "## 2. Data Validation Requirements\n",
    "\n",
    "### 2.1 Schema Validation\n",
    "- Verify column count matches expected schema\n",
    "- Validate column names (case-sensitive/insensitive options)\n",
    "- Support optional and required columns\n",
    "- Handle column order variations\n",
    "- Validate header row presence/absence\n",
    "- Support custom column mappings\n",
    "- Handle duplicate column names\n",
    "\n",
    "### 2.2 Data Type Validation\n",
    "- Validate and convert to specified data types:\n",
    "  - Integers (with range validation)\n",
    "  - Floating-point numbers (with precision requirements)\n",
    "  - Dates (multiple formats)\n",
    "  - Timestamps (multiple timezone support)\n",
    "  - Boolean values (multiple representations)\n",
    "  - Strings (with length limits)\n",
    "- Handle missing values (NULL, NA, empty strings)\n",
    "- Support custom data type converters\n",
    "- Validate against regular expressions\n",
    "- Check for data consistency within columns\n",
    "\n",
    "### 2.3 Business Rule Validation\n",
    "- Support for custom validation rules\n",
    "- Validate dependencies between columns\n",
    "- Check for unique constraints\n",
    "- Validate against reference data\n",
    "- Support for range checks\n",
    "- Handle conditional validations\n",
    "- Validate aggregated values\n",
    "\n",
    "## 3. Performance Requirements\n",
    "\n",
    "### 3.1 Resource Management\n",
    "- Maximum memory usage: \n",
    "  - Not exceed 80% of available system memory\n",
    "  - Support configurable memory limits\n",
    "- CPU utilization:\n",
    "  - Maximum 70% CPU usage per process\n",
    "  - Support for multi-threading\n",
    "- Disk I/O:\n",
    "  - Buffered reading (configurable buffer size)\n",
    "  - Streaming support for large files\n",
    "  - Minimum disk I/O operations\n",
    "\n",
    "### 3.2 Processing Speed\n",
    "- Process 1 million rows per minute on reference hardware\n",
    "- Maximum latency for error detection: 100ms\n",
    "- Maximum initialization time: 500ms\n",
    "- Support for batch processing\n",
    "- Asynchronous validation support\n",
    "- Parallel processing capabilities\n",
    "- Lazy loading options for large datasets\n",
    "\n",
    "### 3.3 Scalability\n",
    "- Linear scaling with file size\n",
    "- Support horizontal scaling\n",
    "- Handle multiple files simultaneously\n",
    "- Support distributed processing\n",
    "- Queue management for multiple requests\n",
    "\n",
    "## 4. Error Handling Requirements\n",
    "\n",
    "### 4.1 Error Detection\n",
    "- Detect and categorize errors:\n",
    "  - System errors (IO, memory, permissions)\n",
    "  - Data format errors\n",
    "  - Validation errors\n",
    "  - Business rule violations\n",
    "- Support error severity levels\n",
    "- Implement error prioritization\n",
    "- Support custom error categories\n",
    "- Handle cascading errors\n",
    "\n",
    "### 4.2 Error Reporting\n",
    "- Structured error messages containing:\n",
    "  - Error code\n",
    "  - Error category\n",
    "  - Timestamp\n",
    "  - File position (line/column)\n",
    "  - Contextual data\n",
    "  - Suggested resolution\n",
    "- Support multiple output formats:\n",
    "  - JSON\n",
    "  - XML\n",
    "  - Plain text\n",
    "  - Custom formats\n",
    "- Support for internationalization (i18n)\n",
    "\n",
    "### 4.3 Logging Requirements\n",
    "- Log levels: DEBUG, INFO, WARN, ERROR, FATAL\n",
    "- Log rotation and archival\n",
    "- Maximum log file size: 1GB\n",
    "- Log format:\n",
    "  ```\n",
    "  timestamp | level | process_id | thread_id | file | line | message\n",
    "  ```\n",
    "- Support for external logging systems:\n",
    "  - ELK Stack\n",
    "  - Splunk\n",
    "  - CloudWatch\n",
    "- Performance metrics logging\n",
    "- Audit trail logging\n",
    "\n",
    "## 5. Recovery Requirements\n",
    "\n",
    "### 5.1 Error Recovery\n",
    "- Implement automatic retry logic:\n",
    "  - Maximum 3 retries\n",
    "  - Exponential backoff\n",
    "  - Configurable retry intervals\n",
    "- Support partial file processing\n",
    "- Implement checkpointing\n",
    "- Support transaction rollback\n",
    "- Maintain data consistency during recovery\n",
    "- Support for resume-able operations\n",
    "\n",
    "### 5.2 Fallback Mechanisms\n",
    "- Alternative data source support\n",
    "- Cached data usage\n",
    "- Default value handling\n",
    "- Support for degraded operation modes\n",
    "- Circuit breaker implementation\n",
    "\n",
    "## 6. Integration Requirements\n",
    "\n",
    "### 6.1 API Requirements\n",
    "- Clean, well-documented API\n",
    "- Support for callback functions\n",
    "- Event-driven architecture\n",
    "- Support for middleware\n",
    "- Pluggable components\n",
    "- Configuration management\n",
    "- Version compatibility\n",
    "\n",
    "### 6.2 Monitoring Integration\n",
    "- Support for health checks\n",
    "- Performance metrics exposure\n",
    "- Error rate monitoring\n",
    "- Resource usage tracking\n",
    "- Integration with monitoring tools:\n",
    "  - Prometheus\n",
    "  - Grafana\n",
    "  - Custom monitoring solutions\n",
    "\n",
    "## 7. Documentation Requirements\n",
    "- API documentation\n",
    "- Error code reference\n",
    "- Configuration guide\n",
    "- Troubleshooting guide\n",
    "- Performance tuning guide\n",
    "- Best practices guide\n",
    "- Sample implementations\n",
    "- Migration guide\n",
    "\n",
    "## 8. Testing Requirements\n",
    "- Unit test coverage: minimum 90%\n",
    "- Integration test coverage: minimum 80%\n",
    "- Performance test suite\n",
    "- Stress test scenarios\n",
    "- Error simulation capabilities\n",
    "- Regression test suite\n",
    "- Documentation for test cases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from typing import Optional, Dict, Any\n",
    "import logging\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Specification\n",
    "### High-Level Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "\n",
    "graph TD\n",
    "    A[Input CSV] --> B[Validation Layer]\n",
    "    B --> C[Error Handler]\n",
    "    C --> D[Logger]\n",
    "    B --> E[Data Processor]\n",
    "    E --> F[Output DataFrame]\n",
    "    E --> C\n",
    "    C --> G[Error Recovery]\n",
    "    G --> B\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Hierarchy\n",
    "\n",
    "```mermaid\n",
    "\n",
    "graph TD\n",
    "    A[BaseException] --> B[Exception]\n",
    "    B --> C[CSVError]\n",
    "    C --> D[FileError]\n",
    "    C --> E[ValidationError]\n",
    "    C --> F[ProcessingError]\n",
    "    D --> G[FileNotFoundError]\n",
    "    D --> H[PermissionError]\n",
    "    E --> I[SchemaError]\n",
    "    E --> J[DataTypeError]\n",
    "    F --> K[MemoryError]\n",
    "    F --> L[EncodingError]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "### Core Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVReadingError(Exception):\n",
    "    \"\"\"Custom exception for CSV reading errors\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_csv_structure(df: pd.DataFrame, expected_columns: list) -> bool:\n",
    "    \"\"\"\n",
    "    Validate the structure of the CSV file.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to validate\n",
    "        expected_columns: List of expected column names\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if validation passes, False otherwise\n",
    "    \"\"\"\n",
    "    if not all(col in df.columns for col in expected_columns):\n",
    "        missing_cols = [col for col in expected_columns if col not in df.columns]\n",
    "        raise CSVReadingError(f\"Missing required columns: {missing_cols}\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_types(df: pd.DataFrame, dtype_map: Dict[str, Any]) -> bool:\n",
    "    \"\"\"\n",
    "    Verify data types of columns.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to check\n",
    "        dtype_map: Dictionary mapping column names to expected data types\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if all types match, False otherwise\n",
    "    \"\"\"\n",
    "    for col, dtype in dtype_map.items():\n",
    "        try:\n",
    "            df[col] = df[col].astype(dtype)\n",
    "        except (ValueError, TypeError) as e:\n",
    "            raise CSVReadingError(f\"Data type conversion failed for column '{col}': {str(e)}\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_read_csv(\n",
    "    file_path: str,\n",
    "    expected_columns: Optional[list] = None,\n",
    "    dtype_map: Optional[Dict[str, Any]] = None,\n",
    "    **kwargs\n",
    ") -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Safely read a CSV file with comprehensive error handling.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the CSV file\n",
    "        expected_columns: List of expected column names\n",
    "        dtype_map: Dictionary mapping column names to expected data types\n",
    "        **kwargs: Additional arguments to pass to pd.read_csv\n",
    "        \n",
    "    Returns:\n",
    "        Optional[pd.DataFrame]: DataFrame if successful, None if failed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if file exists\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "            \n",
    "        # Check file size\n",
    "        file_size = os.path.getsize(file_path)\n",
    "        if file_size == 0:\n",
    "            raise pd.errors.EmptyDataError(\"File is empty\")\n",
    "            \n",
    "        # Check file permissions\n",
    "        if not os.access(file_path, os.R_OK):\n",
    "            raise PermissionError(f\"No read permission for file: {file_path}\")\n",
    "            \n",
    "        # Try different encodings if not specified\n",
    "        encodings = kwargs.pop('encoding', ['utf-8', 'latin1', 'iso-8859-1'])\n",
    "        if isinstance(encodings, str):\n",
    "            encodings = [encodings]\n",
    "            \n",
    "        df = None\n",
    "        encoding_errors = []\n",
    "        \n",
    "        for encoding in encodings:\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, encoding=encoding, **kwargs)\n",
    "                break\n",
    "            except UnicodeDecodeError as e:\n",
    "                encoding_errors.append(f\"Failed with encoding {encoding}: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "        if df is None:\n",
    "            raise UnicodeDecodeError(\n",
    "                \"Failed to read with all attempted encodings: \" + \n",
    "                \"; \".join(encoding_errors)\n",
    "            )\n",
    "            \n",
    "        # Validate structure if expected_columns provided\n",
    "        if expected_columns:\n",
    "            validate_csv_structure(df, expected_columns)\n",
    "            \n",
    "        # Check data types if dtype_map provided\n",
    "        if dtype_map:\n",
    "            check_data_types(df, dtype_map)\n",
    "            \n",
    "        # Log success\n",
    "        logger.info(f\"Successfully read CSV file: {file_path}\")\n",
    "        logger.info(f\"DataFrame shape: {df.shape}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"File not found error: {str(e)}\")\n",
    "    except pd.errors.EmptyDataError as e:\n",
    "        logger.error(f\"Empty file error: {str(e)}\")\n",
    "    except pd.errors.ParserError as e:\n",
    "        logger.error(f\"Parser error (possibly incorrect delimiter): {str(e)}\")\n",
    "    except UnicodeDecodeError as e:\n",
    "        logger.error(f\"Encoding error: {str(e)}\")\n",
    "    except PermissionError as e:\n",
    "        logger.error(f\"Permission error: {str(e)}\")\n",
    "    except MemoryError as e:\n",
    "        logger.error(f\"Memory error (file too large): {str(e)}\")\n",
    "    except CSVReadingError as e:\n",
    "        logger.error(f\"CSV validation error: {str(e)}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error: {str(e)}\")\n",
    "        \n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Scenarios\n",
    "### Normal Operations\n",
    "### Error Conditions\n",
    "### Edge Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Define expected structure\n",
    "expected_cols = ['date', 'temperature', 'humidity', 'rainfall']\n",
    "dtype_mapping = {\n",
    "    'date': 'datetime64[ns]',\n",
    "    'temperature': 'float64',\n",
    "    'humidity': 'float64',\n",
    "    'rainfall': 'float64'\n",
    "}\n",
    "\n",
    "# Example 1: Reading a valid CSV\n",
    "try:\n",
    "    df = safe_read_csv(\n",
    "        'weather.csv',\n",
    "        expected_columns=expected_cols,\n",
    "        dtype_map=dtype_mapping,\n",
    "        parse_dates=['date']\n",
    "    )\n",
    "    if df is not None:\n",
    "        print(\"Successfully read weather data:\")\n",
    "        print(df.head())\n",
    "except Exception as e:\n",
    "    print(f\"Failed to read weather data: {str(e)}\")\n",
    "\n",
    "# Example 2: Reading a non-existent file\n",
    "df_bad = safe_read_csv('nonexistent.csv')\n",
    "\n",
    "# Example 3: Reading with incorrect data types\n",
    "bad_dtype_mapping = {\n",
    "    'temperature': 'int64'  # This will fail for decimal values\n",
    "}\n",
    "df_bad_types = safe_read_csv(\n",
    "    'weather.csv',\n",
    "    dtype_map=bad_dtype_mapping\n",
    ")\n",
    "\n",
    "# Example 4: Custom delimiter handling\n",
    "df_custom = safe_read_csv(\n",
    "    'weather.csv',\n",
    "    sep=';',  # Try with semicolon delimiter\n",
    "    on_bad_lines='warn'  # Warn about problematic lines\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check CSV file before reading\n",
    "def preview_csv(file_path: str, nrows: int = 5) -> None:\n",
    "    \"\"\"\n",
    "    Preview a CSV file's content before reading it into pandas.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the CSV file\n",
    "        nrows: Number of rows to preview\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            print(f\"\\nPreviewing first {nrows} rows of {file_path}:\")\n",
    "            for i, line in enumerate(file):\n",
    "                if i < nrows:\n",
    "                    print(f\"Row {i + 1}: {line.strip()}\")\n",
    "                else:\n",
    "                    break\n",
    "                    \n",
    "        # Get file info\n",
    "        file_size = os.path.getsize(file_path) / (1024 * 1024)  # Size in MB\n",
    "        print(f\"\\nFile size: {file_size:.2f} MB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error previewing file: {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of preview function\n",
    "preview_csv('weather.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Analysis\n",
    "### Benchmarks\n",
    "\n",
    "* Error detection speed\n",
    "* Recovery time\n",
    "* Logging overhead\n",
    "* Memory usage during error handling\n",
    "\n",
    "### Resource Usage\n",
    "\n",
    "* Memory footprint\n",
    "* CPU utilization\n",
    "* Disk I/O impact\n",
    "* Network impact (if applicable)\n",
    "\n",
    "### Optimization Opportunities\n",
    "\n",
    "* Batch processing\n",
    "* Caching strategies\n",
    "* Resource pooling\n",
    "* Async error handling\n",
    "\n",
    "## References\n",
    "### Citations\n",
    "\n",
    "Python Documentation: Error Handling\n",
    "\n",
    "https://docs.python.org/3/tutorial/errors.html\n",
    "\n",
    "\n",
    "Pandas Documentation: IO Tools\n",
    "\n",
    "https://pandas.pydata.org/docs/user_guide/io.html\n",
    "\n",
    "\n",
    "Python Logging Documentation\n",
    "\n",
    "https://docs.python.org/3/library/logging.html\n",
    "\n",
    "\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "* PEP 8 - Style Guide for Python Code\n",
    "* PEP 20 - The Zen of Python\n",
    "* SOLID Principles\n",
    "* Clean Code principles for error handling\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "* Error handling patterns\n",
    "* Logging best practices\n",
    "* Testing strategies\n",
    "* Performance optimization techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
